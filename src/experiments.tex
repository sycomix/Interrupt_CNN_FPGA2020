\label{sec:experiments}

In this section, the evaluation of the instruction-based-interruption, hardware modules for post-processing, and the overall MA-Explore system are presented and analyzed.

\subsection{ Instruction-based-interruption}

The proposed interruptible is implemented and evaluated on the Xilinx ZCU102 evaluation board \cite{zcu102}. The CNN accelerator is developed based on the Xilinx AI accelerator, DPU \cite{dpu}. 

% The performance and the latency of the interruptible DPU and the not-interruptible DPU are listed in \Cref{tab:anywhere}. 
In MR-Explore, only the low-priority PR task is interruptible, and the interrupt position is unpredictable.
We record some of the interrupt locations when running MR-Explore on the interruptible DPU. The performance and the latency of the interruptible DPU and the not-interruptible DPU are listed in \Cref{tab:anywhere}. 
The execution time (Exe time) in \Cref{tab:anywhere} shows the time to complete two tasks, a interruptible PR task and a FE task.

The 'Serial' row is the baseline for executing the two tasks in serial. In serial execution, backup/recovery of data is not required. The 'CPU-like' row is the estimated result of backup/recovery all the on-chip caches for immediately interruption. The 'Pose 1,2,3' rows are the results of the DPU interrupted at different  PR positions. 'Pose 1' represents the first layers of PR network, the numbers of input channel and output channel are both $64$. The input and output shapes are both $240 \times 320$. 'Pose 2' represents the layers with moderate number of channels ($ 512 $) and data shapes ($ 60 \times 80 $). 'Pose 3' represents the last layers with many channels ($ 2048 $) and small featuremaps ($ 15 \times 20$).

DPU first calculates all channels of the output row before calculating the next rows. As the number of channels increases, the number of weights requiring recovery increases squarely. However, the size of the input data to be restored and the output results to be backed up remains basically the same.


% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table}[t]
  \small
  \centering
  \caption{Pref. for different interrupt positions.}
    % Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{tabular}{|c|c|c|c|c|c|}
  \hline
        & Backup  & Recovery & Exe time & Perf. & Latency \bigstrut[t]\\
        & (KB)  & (KB)  & (ms)  & Reduce & (ms) \bigstrut[b]\\
  \hline
  Pose 1 &       &       &       &       &  \bigstrut\\
  \hline
  Pose 2 &       &       &       &       &  \bigstrut\\
  \hline
  Pose 3 &       &       &       &       &  \bigstrut\\
  \hline
  CPU-Like & 4000  & 4000  &       &       & <1 \bigstrut\\
  \hline
  Serial & -     & -     &       &  baseline & - \bigstrut\\
  \hline
  \end{tabular}%
  
  
  \label{tab:anywhere}%
\end{table}%



% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[t]
%     \centering
%     \caption{Interrupt after complete results vs Interrupt anywhere}
% % Table generated by Excel2LaTeX from sheet 'Sheet2'
% \begin{tabular}{|c|c|c|c|c|c|}
%   \hline
%   \multicolumn{1}{|c}{} &       & \multicolumn{1}{c|}{Backup } & \multicolumn{1}{c|}{Recovery} & Exe time & Performance \bigstrut[t]\\
%   \multicolumn{1}{|c}{} &       & \multicolumn{1}{c|}{data (KB)} & \multicolumn{1}{c|}{ data (KB)} & (ms)  & Reduce \bigstrut[b]\\
%   \hline
%   \multicolumn{1}{|p{3.315em}|}{Inter } & \multicolumn{1}{p{3.69em}|}{AfterSave} &       &       &       &  \bigstrut\\
%   \cline{2-6}\multicolumn{1}{|p{3.315em}|}{position 1} & Anyware &       &       &       &  \bigstrut\\
%   \hline
%   \multicolumn{1}{|p{3.315em}|}{Inter } & \multicolumn{1}{p{3.69em}|}{AfterSave} &       &       &       &  \bigstrut\\
%   \cline{2-6}\multicolumn{1}{|p{3.315em}|}{ position 2} & Anyware &       &       &       &  \bigstrut\\
%   \hline
%   \multicolumn{1}{|p{3.315em}|}{Inter } & \multicolumn{1}{p{3.69em}|}{AfterSave} &       &       &       &  \bigstrut\\
%   \cline{2-6}\multicolumn{1}{|p{3.315em}|}{position 3} & Anyware &       &       &       &  \bigstrut\\
%   \hline
%   \multicolumn{2}{|p{7.005em}|}{CPU-Like} &       &       &       &  \bigstrut\\
%   \hline
%   \multicolumn{1}{|c}{} &       & \multicolumn{1}{c|}{Instruction} & \multicolumn{1}{c|}{Latency} & Exe time & Performance \bigstrut[t]\\
%   \multicolumn{1}{|c}{} &       & \multicolumn{1}{c|}{ (KB)} & \multicolumn{1}{c|}{(ms)} & (ms)  & Reduce \bigstrut[b]\\
%   \hline
%   \multicolumn{1}{|c|}{No} & \multicolumn{1}{p{3.69em}|}{Origin} &       &       &       & 0 \bigstrut\\
%   \cline{2-6}\multicolumn{1}{|c|}{ Interrupt} & \multicolumn{1}{p{3.69em}|}{After results} &       &       &       &  \bigstrut\\
%   \cline{2-6}      & Anyware &       &       &       &  \bigstrut\\
%   \hline
%   \end{tabular}%
  
%     \label{tab:anywhere}%
%   \end{table}%



\subsection{ Place Recognition With DPU }

In this section, we design experiments to evaluate the accuracy and efficiency of the PR network on DPU.

\subsubsection{accuracy}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{fig/result.eps}
    \caption{Precision-Recall curve on Citycenter dataset}
    \label{fig:PRcurve}
\end{figure}

We test our network on Citycenter dataset \cite{citycenter} and draw the precision-recall curve. We then compare the result with another network, NetVLAD \cite{netvlad} to show our network can get better performance. We further compare results between before and after optimization and the result doesn't go down much.

In figure \ref{fig:PRcurve}, it's clear that GeM netowrk run better than NetVLAD in all situations. After quantization, the accuracy goes down a bit, especially when precision is high. But our quantized network still outperforms NetVLAD in most cases.

\subsubsection{efficiency}

\begin{table}
    \label{tab:gem_eff}
    \centering 
    \caption{runtime comparison of each operation in PR}
    \begin{tabular}{|c|c|c|}
				\hline
              & gem pooling & normalization \\
        \hline
        CPU   &   ms &   ms \\
        \hline
        CPU+FPGA &   ms &   ms \\
			  \hline
    \end{tabular}
  \end{table}

We compare running time of each operation in GeM network and the results is shown in table \ref{tab:gem_eff}. After optimization, the total time reduces by ?\%.


\subsection{ VO With DPU }

To evaluate the performance of the SuperPoint network in visual odometer, we experimented on the $TUM$ dataset. We evaluate SuperPoint against two well-known detector and descriptor systems: SIFT\cite{Lowe-478} and ORB\cite{RubleeRabaud-479}. We apply the three systems to the visual odometer. We also evaluate the performance after optimization. We compute a maximum of 200 points for all systems at a $480\times640$ resolution and set $NMS=4$. We perform nearest neighbor matching from descriptors in adjacent frames with a maximum allowable distance $d_m$. $d_m$ is not same in three system because descriptors are not in the same order of magnitude. We use an OpenCV implementation (solvePnP()) with all the matches to compute the transform matrix, and use Bundle Adjustment to optimize results. All the computation of this experiment is all done on the CPU except CNN of SuperPoint. 

\begin{table}[t]
  \centering
  \caption{ Accuracy and runtime results on the TUM\cite{sturm12iros} SLAM dataset  }
  \footnotesize
  \begin{threeparttable}
% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{tabular}{|c|c|c|c|c|} 
  \hline
        & \multirow{2}[2]{*}{$d_m$$^1$} & RPE$^2$ & ATE$^3$  & Run  \bigstrut[t]\\
        &       &  (m/s) & (m) & time(ms) \bigstrut[b]\\
  \hline
  SIFT  & 200   & 0.0319  & 0.4219 & 2397  \bigstrut\\
  \hline
  ORB   & 30    & 0.0577  & 0.6105 & 229  \bigstrut\\
  \hline
  Origin & \multirow{2}[2]{*}{0.7} & \multirow{2}[2]{*}{0.0280} & \multirow{2}[2]{*}{0.3671} & \multirow{2}[2]{*}{259} \bigstrut[t]\\
   Superpoint &       &       &       &  \bigstrut[b]\\
  \hline
  Our Fixed & \multirow{2}[2]{*}{360} & \multirow{2}[2]{*}{0.0283} & \multirow{2}[2]{*}{0.3976} & \multirow{2}[2]{*}{59} \bigstrut[t]\\
   Superpoint &       &       &       &  \bigstrut[b]\\
  \hline
  \end{tabular}%
  

\begin{tablenotes}
  \item[1] $d_m$ is the maximum allowable distance between matched descriptors.  
  \item[2] RPE is the mean Relative Pose Error to indicate the translational drift per second, the less, the better.
  \item[3] ATE is the root mean square Absolute Trajectory Error to indicate the translational drift of the entire trajectory, the less, the better.
\end{tablenotes}
    \end{threeparttable}
  \label{tab:VO}%
\end{table}%

Results are shown in \Cref{tab:VO}. In terms of accuracy, SuperPoint outperforms ORB and performs comparably to SIFT. Optimization does not introduce a large loss of accuracy. In terms of calculation speed, SuperPoint takes less time than Sift, and is equivalent to Orb. After optimization, the running speed is increased by $4\times$, making real-time processing possible.

\begin{table}[t]
  \centering
  \caption{runtime Comparison of each operation}
% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{tabular}{|c|c|c|c|c|}
  \hline
             &    softmax &        NMS &       sort &  normalize \bigstrut\\
  \hline
         CPU &       31ms &       27ms &       0.97ms &       42ms \bigstrut\\
  \hline
    CPU+FPGA &     1.97ms &      0.7ms &     0.12ms &     1.44ms \bigstrut\\
  \hline
  \end{tabular}  
  
  \label{tab:optimization}%
\end{table}%

We compare the running time of each operation in SuperPoint before and after the optimization. Results are shown in \Cref{tab:optimization}. The running time of each operation is reduced by more than $15\times$. There is a certain gap between the experimental results of the acceleration effect and the theoretical derivation in \Cref{sec:software}. The possible reason is that the CPU needs time to schedule the FPGA accelerator.

\subsection{ ROS based MA-Explore }